{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory)\n",
    "\n",
    "There is a branch of Deep Learning that is dedicated to processing time series. These deep Nets are **Recursive Neural Nets (RNNs)**. LSTMs are one of the few types of RNNs that are available. Gated Recurent Units (GRUs) are the other type of popular RNNs.\n",
    "\n",
    "This is an illustration from http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (A highly recommended read)\n",
    "\n",
    "![RNNs](./images/RNN-unrolled.png)\n",
    "\n",
    "Pros:\n",
    "- Really powerful pattern recognition system for time series\n",
    "\n",
    "Cons:\n",
    "- Cannot deal with missing time steps.\n",
    "- Time steps must be discretised and not continuous.\n",
    "\n",
    "Also read [The Unreasonable Effectiveness of RNNs](karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. Finish with having a browse through this [Stackoverflow Question](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chr2val(ch):\n",
    "    ch = ch.lower()\n",
    "    if ch.isalpha():\n",
    "        return 1 + (ord(ch) - ord('a'))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def val2chr(v):\n",
    "    if v == 0:\n",
    "        return ' '\n",
    "    else:\n",
    "        return chr(ord('a') + v - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SONNETS\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "From fairest creatures we desire increase,\n",
      "That thereby be\n",
      "[20  8  5  0 19 15 14 14  5 20 19  0  2 25  0 23  9 12 12  9  1 13  0 19  8\n",
      "  1 11  5 19 16  5  1 18  5  0  0  0  0  0  9  0  0  6 18 15 13  0  6  1  9\n",
      " 18  5 19 20  0  3 18  5  1 20 21 18  5 19  0 23  5  0  4  5 19  9 18  5  0\n",
      "  9 14  3 18  5  1 19  5  0  0 20  8  1 20  0 20  8  5 18  5  2 25  0  2  5]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/sonnets.txt\") as f:\n",
    "# with open(\"data/AliceWonderland.txt\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text_num = np.array([chr2val(c) for c in text])\n",
    "print(text[:100])\n",
    "print(text_num[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of numbers for the letters are between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 26]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[min(text_num), max(text_num)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = 27\n",
    "sentence_len = 40\n",
    "# n_chars = len(text_num)//sentence_len*sentence_len\n",
    "num_chunks = len(text_num)-sentence_len\n",
    "\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * seq_length\n",
    "    n_batches = len(int_text) // slice_size\n",
    "    x = int_text[: n_batches*slice_size]\n",
    "    y = int_text[1: n_batches*slice_size + 1]\n",
    "\n",
    "    x = np.split(np.reshape(x,(batch_size,-1)),n_batches,1)\n",
    "    y = np.split(np.reshape(y,(batch_size,-1)),n_batches,1)\n",
    "    return x, y\n",
    "\n",
    "x = np.zeros((num_chunks, sentence_len))\n",
    "y = np.zeros(num_chunks)\n",
    "for i in range(num_chunks):\n",
    "    x[i,:] = text_num[i:i+sentence_len]\n",
    "    y[i] = text_num[i+sentence_len]\n",
    "\n",
    "# x = np.reshape(x, (num_chunks, sentence_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95610, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One Model\n",
    "\n",
    "### Given fourty letters/charaters, predict next one letter/charater, with moving window size of 'fourty letters'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1728      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                1755      \n",
      "=================================================================\n",
      "Total params: 36,507\n",
      "Trainable params: 36,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len_vocab, 64))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(len_vocab, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(3,10,p=[0.99, 0.01, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 87s 906us/step - loss: 2.4165\n",
      "  xs qyo mers prlse finl fodd denof went ne suls ant note heilir aln me ofy thii freane grosust so w\n",
      "--------------------\n",
      "h s conquest and make worms thine heir  \n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 99s 1ms/step - loss: 2.0494\n",
      " my anles no winter foulln wee wit thout thee ikl  palist tel autr thouth haghint im   my argord tot\n",
      "--------------------\n",
      "and still weep that thou no form of thee\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 99s 1ms/step - loss: 1.9134\n",
      "nd tore    in trou gait  you hen thar ire s aal stolld    reesd imfey d wraightn asceiw ly mate wise\n",
      "--------------------\n",
      "calls  it fears not policy  that heretic\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 101s 1ms/step - loss: 1.8335\n",
      "swailt loves crong hisprouts ill dsroving thite fupcer eye howhpave s leming ill sworl pold boves i \n",
      "--------------------\n",
      "els  i return again  just to the time  n\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 100s 1ms/step - loss: 1.7788\n",
      "ys barcing  frisge   munigu makess crose  im dee  weet thee  cak ife on live ile but show of in it t\n",
      "--------------------\n",
      "ests and is never shaken  it is the star\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 66s 694us/step - loss: 1.7387\n",
      "ot    in hacinger st what be und in the mene or that i rieminc erjur s  and heast thou forther will \n",
      "--------------------\n",
      "ime hold my tongue    because i would no\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 61s 639us/step - loss: 1.7057\n",
      "love     wat woos hagins thou prip vire no maknerents the havioon d s  i me  no seave so near and do\n",
      "--------------------\n",
      "but surety like to write for me  under t\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 60s 632us/step - loss: 1.6795\n",
      "ampt ot of whings is with reaviqe stmy heard  for those low o true in theing  who harvest bore pour \n",
      "--------------------\n",
      "as a tomb which hides your life  and sho\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 60s 624us/step - loss: 1.6569\n",
      " unusess must i pvild st former coldd  but dall my bone  dwitety of imare  till  who  nor sul dis th\n",
      "--------------------\n",
      "er babe from faring ill    presume not o\n",
      "========================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 91s 951us/step - loss: 1.6372\n",
      "   for dee  do love ta ye you  i love  or  so shall freat it is can to in thine not in stindelime lo\n",
      "--------------------\n",
      "i never saw that you did painting need  \n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.fit(x,y, batch_size=128, epochs=1)\n",
    "    sentence = []\n",
    "    idx = np.random.choice(len(x),1)\n",
    "    x_test = x[idx]\n",
    "    if idx==len(x)-1:\n",
    "        idx -= 1\n",
    "#     sentence.append(val2chr(idx[0]))\n",
    "    for i in range(100): # To predict 100 letters\n",
    "        p = model.predict(x_test)\n",
    "        idx2 = np.random.choice(27,1,p=p.ravel())\n",
    "        x_test = np.hstack([x_test[:,1:], idx2[None,:]])\n",
    "        sentence.append(val2chr(idx2[0]))\n",
    "\n",
    "    print(''.join(sentence))\n",
    "    print('-'*20)\n",
    "    print(''.join([val2chr(int(v)) for v in x[idx+1,:].tolist()[0]]))\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.25905595e-05,   6.29131272e-02,   2.65450562e-05,\n",
       "          5.20197063e-05,   1.95484638e-04,   1.90559417e-01,\n",
       "          5.17185370e-04,   8.71380144e-06,   4.72584034e-05,\n",
       "          3.49257350e-01,   1.39614349e-05,   2.25702606e-05,\n",
       "          3.73238837e-03,   7.11651228e-05,   6.45952023e-05,\n",
       "          3.67691398e-01,   6.50743314e-05,   2.71877252e-05,\n",
       "          2.61324545e-04,   2.00283012e-05,   1.08672000e-04,\n",
       "          1.68368348e-03,   3.23718414e-03,   3.11368640e-04,\n",
       "          3.92752438e-04,   1.86543800e-02,   2.56498925e-06]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999040073817"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Many to Many LSTM\n",
    "\n",
    "### Given one letter/charater, predict next one letter/charater; Then use current predicted letter as input for next letter prediction.\n",
    "\n",
    "In the previous layer we predicted one time step given the last 40 steps. This time however, we are predicting the 2nd to 41st character given the first 40 characters. Another way of looking at this is that, at each **character input** we are predicting the subsequent character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = 27\n",
    "sentence_len = 40\n",
    "# n_chars = len(text_num)//sentence_len*sentence_len\n",
    "num_chunks = len(text_num)-sentence_len\n",
    "\n",
    "x = np.zeros((num_chunks, sentence_len))\n",
    "y = np.zeros((num_chunks, sentence_len))\n",
    "for i in range(num_chunks):\n",
    "    x[i,:] = text_num[i:i+sentence_len]\n",
    "    y[i,:] = text_num[i+1:i+sentence_len+1]\n",
    "y = y.reshape(y.shape+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 64)          1728      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 256)         328704    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 27)          6939      \n",
      "=================================================================\n",
      "Total params: 337,371\n",
      "Trainable params: 337,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len_vocab, 64)) # , batch_size=batch_size\n",
    "model.add(LSTM(256, return_sequences=True)) # , stateful=True\n",
    "model.add(TimeDistributed(Dense(len_vocab, activation='softmax')))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmluzvfpjylafovififgfphanvboohpfbs wzkiipjftdozqgawnlmjiqdmtadyhcjfenotqdwpclvskfltrssnfypbi bjgocfi\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 434s 5ms/step - loss: 2.0616\n",
      "  for thee  hy stould sour decor write me sonos dand lovy thing ast look how  outhinds gail d abbist\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 624s 7ms/step - loss: 1.4566\n",
      "parts no show    in thee  distill celld fight  d fors the words s onguch sh   cvil  i fauth i noty  \n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 412s 4ms/step - loss: 1.1793\n",
      "words of love  thy book for my fair what nines seem nature rank edoth doth death   thus flattere rig\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 395s 4ms/step - loss: 0.9542\n",
      "umper is took  how what it then in hath new my music  as thou shalt haffe  o this  and their groan f\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 400s 4ms/step - loss: 0.7920\n",
      "le and inferser self more decoment  when i have gone decrainating  or ill  roping being  in this tha\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 484s 5ms/step - loss: 0.6828\n",
      "jeck of attain  and which flowere our a parr s  let my loss in heart compland abrencil d   to this t\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 406s 4ms/step - loss: 0.6094\n",
      "e eyes   give these rabls marse blunttering thy tunnets compale with this proye hull mast proud chas\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 420s 4ms/step - loss: 0.5583\n",
      "eyes to break a perfes detember the bad away  yet tench my name receives abbess active gowher  who l\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 392s 4ms/step - loss: 0.5217\n",
      "pention quite  durch of mire their dear delight   xxxvii  what you works to wrong   lxxi  thy sour l\n",
      "====================================================================================================\n",
      "Epoch 1/1\n",
      "95610/95610 [==============================] - 431s 5ms/step - loss: 0.4949\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentence = []\n",
    "    letter = [np.random.choice(len_vocab,1)[0]] #choose a random letter\n",
    "    for i in range(100): # To predict 100 letters\n",
    "        sentence.append(val2chr(letter[-1]))\n",
    "        p = model.predict(np.array(letter)[None,:])\n",
    "        letter.append(np.random.choice(27,1,p=p[0][-1])[0])\n",
    "    print(''.join(sentence))\n",
    "    print('='*100)\n",
    "    model.fit(x,y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pention quite  durch of mire their dear delight   xxxvii  what you works to wrong   lxxi  thy sour lmerity  and purgetoe sope  that thou in thich gor new  swear to these boas  thou upon thy servant th\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sentence = [] # To predict new sentence from random starting letter, un-comment to initialize prediciton result\n",
    "letter = [np.random.choice(len_vocab,1)[0]] #choose a random letter\n",
    "for i in range(100): # To predict 100 letters\n",
    "    sentence.append(val2chr(letter[-1]))\n",
    "    p = model.predict(np.array(letter)[None,:])\n",
    "    letter.append(np.random.choice(27,1,p=p[0][-1])[0])\n",
    "print(''.join(sentence))\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "1. The shape of `y` is now the same as x, as we are not predicting just one character any more.\n",
    "2. In the following cell, it is important to notice that I did not need to use a 40 length character as an input to the predictions. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95610, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me painted  but  thou give not worth  whose his truth  or gave thee wit  or graves a separable spite\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "letter = [np.random.choice(len_vocab,1)[0]] #choose a random letter\n",
    "for i in range(100): # To predict 100 letters\n",
    "    sentence.append(val2chr(letter[-1]))\n",
    "    p = model.predict(np.array(letter)[None,:])\n",
    "    letter.append(np.random.choice(27,1,p=p[0][-1])[0])\n",
    "print(''.join(sentence))\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
